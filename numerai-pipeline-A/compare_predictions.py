"""
æ¯”è¼ƒåŸå§‹25å€‹æ¨¡å‹é›†æˆæ–¹æ³•èˆ‡æœ€çµ‚æ¨¡å‹çš„é æ¸¬çµæœ
ç¢ºä¿å…©ç¨®æ–¹æ³•ç”¢ç”Ÿä¸€è‡´çš„çµæœ
"""

import os
import sys
import json
import joblib
import numpy as np
import pandas as pd
import cloudpickle
from loguru import logger
from scipy import stats

# æ·»åŠ srcç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from preprocess import rank_gauss
from neutralize import neutralize


def neutralize_single_era(predictions, features, neutralizers, proportion=0.85):
    """
    å°å–®ä¸€eraé€²è¡Œneutralizationï¼ˆä¸ä¾è³´eraåˆ†çµ„ï¼‰
    é€™èˆ‡create_final_official_model.pyä¸­çš„æ–¹æ³•ä¸€è‡´
    """
    scores = predictions.rank(method="first").values
    scores = stats.norm.ppf((scores - 0.5) / scores.shape[0])
    exposures = features[neutralizers].fillna(features[neutralizers].median()).values
    scores = scores - proportion * exposures @ np.linalg.pinv(exposures, rcond=1e-6) @ scores
    scores = scores / np.std(scores, axis=0, ddof=0)
    return pd.Series(scores.flatten(), index=predictions.index)


def load_original_models():
    """è¼‰å…¥åŸå§‹çš„25å€‹LightGBMæ¨¡å‹å’ŒPCAæ¨¡å‹"""
    models_dir = "models"
    
    # è¼‰å…¥PCAæ¨¡å‹
    pca_model = joblib.load(os.path.join(models_dir, "pca_model.pkl"))
    
    # è¼‰å…¥æ‰€æœ‰LightGBMæ¨¡å‹
    import glob
    model_files = glob.glob(os.path.join(models_dir, "lgbm_seed*.pkl"))
    lgb_models = []
    for model_file in sorted(model_files):
        model = joblib.load(model_file)
        lgb_models.append(model)
    
    return pca_model, lgb_models


def predict_with_original_method(live_features, feature_cols):
    """ä½¿ç”¨åŸå§‹çš„25å€‹æ¨¡å‹é›†æˆæ–¹æ³•é€²è¡Œé æ¸¬"""
    
    # è¼‰å…¥æ¨¡å‹
    pca_model, lgb_models = load_original_models()
    
    # 1. Rank-Gaussianè®Šæ›
    feat_gauss = rank_gauss(live_features[feature_cols])
    
    # 2. PCAé™ç¶­
    feat = pca_model.transform(feat_gauss).astype(np.float32)
    
    # 3. æ¨¡å‹é›†æˆé æ¸¬
    preds = np.zeros(len(live_features), dtype=np.float32)
    for model in lgb_models:
        preds += model.predict(feat) / len(lgb_models)
    
    # 4. è½‰æ›ç‚ºpandas Seriesé€²è¡Œneutralization
    pred_series = pd.Series(preds, index=live_features.index)
    
    # 5. Neutralizationï¼ˆèˆ‡create_final_official_model.pyä¸€è‡´ï¼šå‰30å€‹ç‰¹å¾µï¼Œproportion=0.85ï¼‰
    neutral_cols = feature_cols[:30]
    neutralized_predictions = neutralize_single_era(
        pred_series, 
        live_features, 
        neutral_cols, 
        proportion=0.85
    )
    
    return neutralized_predictions.values


def predict_with_final_model(live_features, model_file="numerai_final_model_v3.pkl"):
    """ä½¿ç”¨æœ€çµ‚æ¨¡å‹é€²è¡Œé æ¸¬"""
    
    # è¼‰å…¥æœ€çµ‚æ¨¡å‹
    with open(model_file, "rb") as f:
        predict_func = cloudpickle.loads(f.read())
    
    # å‰µå»ºç©ºçš„benchmark_modelsï¼ˆå®˜æ–¹æ ¼å¼è¦æ±‚ï¼‰
    live_benchmark_models = pd.DataFrame(index=live_features.index)
    
    # é æ¸¬
    predictions = predict_func(live_features, live_benchmark_models)
    
    return predictions["prediction"].values


def compare_predictions(n_samples=1000, feature_set="medium"):
    """æ¯”è¼ƒå…©ç¨®é æ¸¬æ–¹æ³•çš„çµæœ"""
    
    logger.info(f"æ¯”è¼ƒå…©ç¨®é æ¸¬æ–¹æ³•ï¼Œæ¨£æœ¬æ•¸: {n_samples}, ç‰¹å¾µé›†: {feature_set}")
    
    # è¼‰å…¥ç‰¹å¾µå…ƒæ•¸æ“š
    with open("data/features.json", 'r') as f:
        feature_metadata = json.load(f)
    features = feature_metadata["feature_sets"][feature_set]
    feature_cols = [c for c in features if c.startswith("feature_")]
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    np.random.seed(42)  # ç¢ºä¿å¯é‡ç¾æ€§
    test_live_features = pd.DataFrame(
        np.random.randn(n_samples, len(features)),
        columns=features,
        index=range(n_samples)
    )
    
    logger.info("ä½¿ç”¨åŸå§‹25å€‹æ¨¡å‹é›†æˆæ–¹æ³•é€²è¡Œé æ¸¬...")
    original_predictions = predict_with_original_method(test_live_features, feature_cols)
    
    logger.info("ä½¿ç”¨æœ€çµ‚æ¨¡å‹é€²è¡Œé æ¸¬...")
    final_predictions = predict_with_final_model(test_live_features)
    
    # æ¯”è¼ƒçµæœ
    logger.info("æ¯”è¼ƒé æ¸¬çµæœ...")
    
    # è¨ˆç®—å·®ç•°çµ±è¨ˆ
    diff = original_predictions - final_predictions
    mae = np.mean(np.abs(diff))
    mse = np.mean(diff**2)
    rmse = np.sqrt(mse)
    max_diff = np.max(np.abs(diff))
    correlation = np.corrcoef(original_predictions, final_predictions)[0, 1]
    
    logger.info("="*50)
    logger.info("é æ¸¬çµæœæ¯”è¼ƒ:")
    logger.info(f"  å¹³å‡çµ•å°èª¤å·® (MAE): {mae:.8f}")
    logger.info(f"  å‡æ–¹èª¤å·® (MSE): {mse:.8f}")
    logger.info(f"  å‡æ–¹æ ¹èª¤å·® (RMSE): {rmse:.8f}")
    logger.info(f"  æœ€å¤§çµ•å°å·®ç•°: {max_diff:.8f}")
    logger.info(f"  ç›¸é—œä¿‚æ•¸: {correlation:.8f}")
    logger.info("="*50)
    
    # çµ±è¨ˆæ‘˜è¦
    logger.info("åŸå§‹æ–¹æ³•é æ¸¬çµ±è¨ˆ:")
    logger.info(f"  å¹³å‡å€¼: {np.mean(original_predictions):.6f}")
    logger.info(f"  æ¨™æº–å·®: {np.std(original_predictions):.6f}")
    logger.info(f"  æœ€å°å€¼: {np.min(original_predictions):.6f}")
    logger.info(f"  æœ€å¤§å€¼: {np.max(original_predictions):.6f}")
    
    logger.info("æœ€çµ‚æ¨¡å‹é æ¸¬çµ±è¨ˆ:")
    logger.info(f"  å¹³å‡å€¼: {np.mean(final_predictions):.6f}")
    logger.info(f"  æ¨™æº–å·®: {np.std(final_predictions):.6f}")
    logger.info(f"  æœ€å°å€¼: {np.min(final_predictions):.6f}")
    logger.info(f"  æœ€å¤§å€¼: {np.max(final_predictions):.6f}")
    
    # åˆ¤æ–·æ˜¯å¦ä¸€è‡´
    if mae < 1e-6 and correlation > 0.999:
        logger.info("âœ… å…©ç¨®æ–¹æ³•çš„é æ¸¬çµæœé«˜åº¦ä¸€è‡´ï¼")
        return True
    elif mae < 1e-4 and correlation > 0.99:
        logger.info("âš ï¸ å…©ç¨®æ–¹æ³•çš„é æ¸¬çµæœåŸºæœ¬ä¸€è‡´ï¼Œå­˜åœ¨å¾®å°å·®ç•°")
        return True
    else:
        logger.error("âŒ å…©ç¨®æ–¹æ³•çš„é æ¸¬çµæœå­˜åœ¨æ˜é¡¯å·®ç•°ï¼")
        return False


if __name__ == "__main__":
    import sys
    
    # è¨­ç½®æ—¥èªŒ
    logger.add("logs/compare_predictions.log", rotation="50 MB", level="INFO")
    
    # å‰µå»ºæ—¥èªŒç›®éŒ„
    os.makedirs("logs", exist_ok=True)
    
    # æª¢æŸ¥åƒæ•¸
    n_samples = int(sys.argv[1]) if len(sys.argv) > 1 else 1000
    feature_set = sys.argv[2] if len(sys.argv) > 2 else "medium"
    
    try:
        success = compare_predictions(n_samples, feature_set)
        if not success:
            sys.exit(1)
        logger.info("ğŸ‰ é æ¸¬æ–¹æ³•ä¸€è‡´æ€§é©—è­‰å®Œæˆï¼")
    except Exception as e:
        logger.error(f"æ¯”è¼ƒå¤±æ•—: {e}")
        sys.exit(1)